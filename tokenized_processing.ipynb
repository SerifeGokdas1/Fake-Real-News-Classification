{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22440 unique tokens.\n",
      "   word_1  word_2  word_3  word_4  word_5  word_6  word_7  word_8  word_9  \\\n",
      "0       0       0       0       0       0       0       0      47      25   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0      26   \n",
      "3       0       0       0       0       0       0       0       0       1   \n",
      "4       0       0       0       0       0       0      99     810       1   \n",
      "\n",
      "   word_10  word_11  word_12  word_13  word_14  word_15  Label  \n",
      "0      393        1        1        1       81        2      0  \n",
      "1        0        0      699      380        1        2      0  \n",
      "2        1      381        1       18        6       14      0  \n",
      "3      536       53        1        1        4        2      0  \n",
      "4        1        1        1        1        1        1      1  \n"
     ]
    }
   ],
   "source": [
    "#Title tokenize etme yani sayısallaştırma \n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "df = pd.read_csv(\"data_all_news.csv\")\n",
    "\n",
    "# Tokenizer nesnesini oluşturma\n",
    "max_words = 1500  # En fazla 2500 kelimeyi dikkate alır\n",
    "oov_tok = \"<OOV>\"  # Sözlükte olmayan kelimeler için kullanılacak token\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=oov_tok)\n",
    "\n",
    "# 'Title' sütunundaki haber başlıkları ile tokenizer'ı eğitiyoruz\n",
    "tokenizer.fit_on_texts(df['Title'])\n",
    "\n",
    "# Başlıkları sayısal dizilere dönüştürme\n",
    "sequences = tokenizer.texts_to_sequences(df['Title'])\n",
    "\n",
    "# Kelime sözlüğünü almak\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(word_index))\n",
    "\n",
    "# Dizileri aynı uzunlukta olacak şekilde doldurmak\n",
    "max_len = 15  # Başlıkların uzunluğunu 20 kelimeye sabitliyoruz\n",
    "X_pad = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Etiketleri (Label) numpy dizisine dönüştürün\n",
    "y = df['Label'].values\n",
    "\n",
    "# X_pad ve y'yi birleştirerek yeni bir DataFrame oluştur\n",
    "df_padded = pd.DataFrame(X_pad, columns=[f\"word_{i+1}\" for i in range(max_len)])\n",
    "\n",
    "# Etiketleri ekleyelim\n",
    "df_padded['Label'] = y\n",
    "\n",
    "# Yeni veriyi yeni bir CSV dosyasına kaydedelim\n",
    "df_padded.to_csv('data_tokenized2.csv', index=False)\n",
    "\n",
    "# İlk birkaç satırı kontrol et\n",
    "print(df_padded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Terminalden huggingface-cli login komutuyla login oldum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 veri işlendi.\n",
      "200 veri işlendi.\n",
      "300 veri işlendi.\n",
      "400 veri işlendi.\n",
      "500 veri işlendi.\n",
      "600 veri işlendi.\n",
      "700 veri işlendi.\n",
      "800 veri işlendi.\n",
      "900 veri işlendi.\n",
      "1000 veri işlendi.\n",
      "1100 veri işlendi.\n",
      "1200 veri işlendi.\n",
      "1300 veri işlendi.\n",
      "1400 veri işlendi.\n",
      "1500 veri işlendi.\n",
      "1600 veri işlendi.\n",
      "1700 veri işlendi.\n",
      "1800 veri işlendi.\n",
      "1900 veri işlendi.\n",
      "2000 veri işlendi.\n",
      "2100 veri işlendi.\n",
      "2200 veri işlendi.\n",
      "2300 veri işlendi.\n",
      "2400 veri işlendi.\n",
      "2500 veri işlendi.\n",
      "2600 veri işlendi.\n",
      "2700 veri işlendi.\n",
      "2800 veri işlendi.\n",
      "2900 veri işlendi.\n",
      "3000 veri işlendi.\n",
      "3100 veri işlendi.\n",
      "3200 veri işlendi.\n",
      "3300 veri işlendi.\n",
      "3400 veri işlendi.\n",
      "3500 veri işlendi.\n",
      "3600 veri işlendi.\n",
      "3700 veri işlendi.\n",
      "3800 veri işlendi.\n",
      "3900 veri işlendi.\n",
      "4000 veri işlendi.\n",
      "4100 veri işlendi.\n",
      "4200 veri işlendi.\n",
      "4300 veri işlendi.\n",
      "4400 veri işlendi.\n",
      "4500 veri işlendi.\n",
      "4600 veri işlendi.\n",
      "4700 veri işlendi.\n",
      "4800 veri işlendi.\n",
      "4900 veri işlendi.\n",
      "5000 veri işlendi.\n",
      "5100 veri işlendi.\n",
      "5200 veri işlendi.\n",
      "5300 veri işlendi.\n",
      "5400 veri işlendi.\n",
      "5500 veri işlendi.\n",
      "5600 veri işlendi.\n",
      "5700 veri işlendi.\n",
      "5800 veri işlendi.\n",
      "5900 veri işlendi.\n",
      "6000 veri işlendi.\n",
      "6100 veri işlendi.\n",
      "6200 veri işlendi.\n",
      "6300 veri işlendi.\n",
      "6400 veri işlendi.\n",
      "6500 veri işlendi.\n",
      "6600 veri işlendi.\n",
      "6700 veri işlendi.\n",
      "6800 veri işlendi.\n",
      "6900 veri işlendi.\n",
      "7000 veri işlendi.\n",
      "7100 veri işlendi.\n",
      "7200 veri işlendi.\n",
      "7300 veri işlendi.\n",
      "7400 veri işlendi.\n",
      "7500 veri işlendi.\n",
      "7600 veri işlendi.\n",
      "7700 veri işlendi.\n",
      "7800 veri işlendi.\n",
      "7900 veri işlendi.\n",
      "8000 veri işlendi.\n",
      "8100 veri işlendi.\n",
      "8200 veri işlendi.\n",
      "8300 veri işlendi.\n",
      "8400 veri işlendi.\n",
      "8500 veri işlendi.\n",
      "8600 veri işlendi.\n",
      "8700 veri işlendi.\n",
      "8800 veri işlendi.\n",
      "8900 veri işlendi.\n",
      "9000 veri işlendi.\n",
      "9100 veri işlendi.\n",
      "9200 veri işlendi.\n",
      "9300 veri işlendi.\n",
      "9400 veri işlendi.\n",
      "9500 veri işlendi.\n",
      "9600 veri işlendi.\n",
      "9700 veri işlendi.\n",
      "9800 veri işlendi.\n",
      "9900 veri işlendi.\n",
      "10000 veri işlendi.\n",
      "10100 veri işlendi.\n",
      "10200 veri işlendi.\n",
      "10300 veri işlendi.\n",
      "10400 veri işlendi.\n",
      "10500 veri işlendi.\n",
      "10600 veri işlendi.\n",
      "10700 veri işlendi.\n",
      "10800 veri işlendi.\n",
      "10900 veri işlendi.\n",
      "11000 veri işlendi.\n",
      "11100 veri işlendi.\n",
      "11200 veri işlendi.\n",
      "11300 veri işlendi.\n",
      "11400 veri işlendi.\n",
      "11500 veri işlendi.\n",
      "11600 veri işlendi.\n",
      "11700 veri işlendi.\n",
      "11800 veri işlendi.\n",
      "11900 veri işlendi.\n",
      "12000 veri işlendi.\n",
      "12100 veri işlendi.\n",
      "12200 veri işlendi.\n",
      "12300 veri işlendi.\n",
      "12400 veri işlendi.\n",
      "12500 veri işlendi.\n",
      "12600 veri işlendi.\n",
      "12700 veri işlendi.\n",
      "Tokenize işlemi tamamlandı ve yeni CSV dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tokenizer'ı başlatıyoruz\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alibayram/tr_tokenizer\", use_fast=True)\n",
    "\n",
    "# CSV dosyasını okuyalım\n",
    "df = pd.read_csv(\"data_all_news.csv\")\n",
    "\n",
    "# Başlıkları (Title) ve etiketleri (Label) alıyoruz\n",
    "titles = df['Title'].tolist()  # 'Title' sütununu al\n",
    "labels = df['Label'].tolist()  # 'Label' sütununu al\n",
    "\n",
    "# Tokenize edilmiş başlıklar ve etiketleri saklamak için liste oluşturuyoruz\n",
    "tokenized_titles = []\n",
    "encoded_labels = []\n",
    "\n",
    "# Tokenize işlemine başlıyoruz\n",
    "for i, (title, label) in enumerate(zip(titles, labels)):\n",
    "    tokens = tokenizer.tokenize(title)  # Başlığı tokenize ediyoruz\n",
    "    tokenized_titles.append(tokens)  # Tokenize edilmiş başlıkları listeye ekliyoruz\n",
    "    encoded_labels.append(label)  # Etiketi listeye ekliyoruz\n",
    "\n",
    "    # Her 100. veri işlendiğinde ekrana yazdırıyoruz\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"{i + 1} veri işlendi.\")\n",
    "\n",
    "# Tokenize edilmiş başlıklar ve etiketlerle DataFrame oluşturuyoruz\n",
    "tokenized_df = pd.DataFrame({\n",
    "    'Title': tokenized_titles,  # Tokenize edilmiş başlıklar\n",
    "    'Label': encoded_labels     # Etiketler\n",
    "})\n",
    "\n",
    "# CSV dosyasına kaydediyoruz\n",
    "tokenized_df.to_csv('data_tokenized.csv', index=False)\n",
    "\n",
    "print(\"Tokenize işlemi tamamlandı ve yeni CSV dosyasına kaydedildi.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
